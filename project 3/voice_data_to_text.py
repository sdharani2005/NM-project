# -*- coding: utf-8 -*-
"""VOICE DATA TO TEXT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWTZRWwxV9rH_l6N7p95HifO30QhNeiB

# **TO CONNECT THE DATASET USING GITHUB IN HEALTHCARE SECTOR SPECIFICALLY DIABETES**
"""

!git clone https://github.com/LIHVOICE/Voice-and-diabetes-VOCADIAB.git

"""# **LIST ALL FILES AND FOLDERS INSIDE DATASET**"""

!ls Voice-and-diabetes-VOCADIAB

!pip install pandas numpy scikit-learn

"""# **LOADING MALE AND FEMALE DIABETES VOICE DATASET [FEW ROWS]**"""

import pandas as pd

# Load both datasets
female_df = pd.read_pickle('Voice-and-diabetes-VOCADIAB/vocadiab_females_dataset.pkl')
male_df = pd.read_pickle('Voice-and-diabetes-VOCADIAB/vocadiab_males_dataset.pkl')

# Combine them
df = pd.concat([female_df, male_df], ignore_index=True)

# Look at first few rows
df.head()

"""# **DISPLAYING ETHNICITY**"""

# @title ethnicity

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('ethnicity').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""# **DISPLAYING COLUMNS**"""

print(df.columns)

"""# DISPLAYING COMMON ROWS"""

print(df.iloc[0])

import numpy as np # Import numpy for vstack

def extract_embedding(row):
    try:
        # Attempt to access the embedding, return if successful
        embedding_data = row['byols_embeddings']
        # Check if it's a dictionary and has the 'embedding' key
        if isinstance(embedding_data, dict) and 'embedding' in embedding_data:
            return embedding_data['embedding']
        else:
            # If not a dictionary or key is missing, handle appropriately
            # This could involve checking for other expected formats
            # or returning NaN if unexpected data is encountered
            return np.nan
    except (KeyError, TypeError):
        # Handle cases where 'byols_embeddings' is missing or invalid
        return np.nan # Return NaN for rows with missing or invalid data

# Get a list of embeddings, filtering out NaN values
embeddings = [emb for emb in df.apply(extract_embedding, axis=1) if not pd.isnull(emb)]

# Check if any embeddings were found
if embeddings:
    X = np.vstack(embeddings) # Only call vstack if embeddings is not empty
else:
    print("No valid embeddings found in the DataFrame.") # Handle the case where no embeddings are found
    X = np.array([]) # Assign an empty array to X if no embeddings were found

import numpy as np
from sklearn.model_selection import train_test_split

# Use correct column names
X = np.vstack(df['byols_embeddings'].values)     # Voice embeddings
y = df['diabetes'].values.astype(int)            # 0 = Non-diabetic, 1 = Diabetic

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""# **FINDING THE ACCURACY FROM VOICE DATASET**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Train
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict & evaluate
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# 1. Train the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# 2. Predict on test set
y_pred = model.predict(X_test)

# 3. Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# **MAKING CONFUSION MATRIX**"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()



import joblib

# Save the trained model to a .pkl file
joblib.dump(model, 'diabetes_voice_model.pkl')

loaded_model = joblib.load('diabetes_voice_model.pkl')

predictions = loaded_model.predict(X_test)

joblib.dump(model, 'diabetes_voice_model.pkl')

!ls

# Load the saved model
loaded_model = joblib.load('diabetes_voice_model.pkl')

# Now you can use it for predictions
predictions = loaded_model.predict(X_test)

!pip install transformers
!pip install soundfile
!pip install torchaudio

from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch
import soundfile as sf

# Load Pretrained Wav2Vec2 model and processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h")

# Function to convert audio file to text
def transcribe_audio(audio_file):
    # Load audio file
    audio_input, _ = sf.read(audio_file)

    # Preprocess audio
    inputs = processor(audio_input, return_tensors="pt", sampling_rate=16000)

    # Make predictions
    with torch.no_grad():
        logits = model(**inputs).logits
        predicted_ids = torch.argmax(logits, dim=-1)

    # Decode the predicted ids to text
    transcription = processor.decode(predicted_ids[0])
    return transcription



"""# **INSTALLING NECESSARY METRICS FOR THE PROJECT**"""

!pip install librosa soundfile speechrecognition pydub nltk scikit-learn matplotlib



!pip install gTTS pydub

from gtts import gTTS
from pydub import AudioSegment

# Step 1: Define a sample sentence
text = "Hello, I want help with my bill. The charges seem wrong."

# Step 2: Convert text to speech
tts = gTTS(text=text, lang='en')
tts.save("customer_query.mp3")

# Step 3: Load and confirm the audio file is created
audio = AudioSegment.from_mp3("customer_query.mp3")
print("MP3 file created successfully.")



from pydub import AudioSegment
import speech_recognition as sr

# Convert MP3 to WAV
audio = AudioSegment.from_mp3("customer_query.mp3")
audio.export("customer_query.wav", format="wav")

# Speech to Text
recognizer = sr.Recognizer()
with sr.AudioFile("customer_query.wav") as source:
    audio_data = recognizer.record(source)
    text = recognizer.recognize_google(audio_data)

print("Customer Said:", text)

import speech_recognition as sr

# Convert MP3 to WAV
audio.export("customer_query.wav", format="wav")

# Transcribe using SpeechRecognition
recognizer = sr.Recognizer()
with sr.AudioFile("customer_query.wav") as source:
    audio_data = recognizer.record(source)
    text = recognizer.recognize_google(audio_data)

print("Transcribed Text:", text)



import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

nltk.download('punkt')
nltk.download('stopwords')

# Sample training data
queries = [
    "I want to pay my bill",
    "Why is my internet not working",
    "Can I speak to an agent",
    "I need help with account settings",
    "What is my current plan"
]

labels = ["billing", "technical", "general", "account", "general"]

# Vectorize and train
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(queries)
model = MultinomialNB()
model.fit(X, labels)

# Predict category of user's query
X_input = vectorizer.transform([text])
predicted = model.predict(X_input)
print("Query Category:", predicted[0])



"""# **VISUALIZATION  OF THE PROJECT**"""

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Load the WAV file you already exported (or use the MP3 if needed)
y, sr = librosa.load("customer_query.wav", sr=16000)

# === Mel Spectrogram ===
mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)

plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spectrogram_db, sr=sr, x_axis='time', y_axis='mel')
plt.colorbar(format='%+2.0f dB')
plt.title('Mel Spectrogram')
plt.tight_layout()
plt.show()

# === MFCC (Mel Frequency Cepstral Coefficients) ===
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

plt.figure(figsize=(10, 4))
librosa.display.specshow(mfccs, sr=sr, x_axis='time')
plt.colorbar()
plt.title('MFCCs')
plt.tight_layout()
plt.show()



# === Waveform ===
plt.figure(figsize=(10, 4))
plt.plot(y)
plt.title('Waveform of Audio Signal')
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.tight_layout()
plt.show()



chromagram = librosa.feature.chroma_stft(y=y, sr=sr)

plt.figure(figsize=(10, 4))
librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', cmap='coolwarm', sr=sr)
plt.colorbar()
plt.title('Chromagram')
plt.tight_layout()
plt.show()



spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
frames = range(len(spectral_centroids))
t = librosa.frames_to_time(frames)

plt.figure(figsize=(10, 4))
plt.semilogy(t, spectral_centroids, label='Spectral Centroid')
plt.xlabel('Time (s)')
plt.ylabel('Hz')
plt.title('Spectral Centroid (Perceived Brightness)')
plt.legend()
plt.tight_layout()
plt.show()

!pip install librosa soundfile speechrecognition pydub nltk scikit-learn matplotlib

!pip install gTTS pydub

from gtts import gTTS
from pydub import AudioSegment

# Step 1: Define a sample sentence
text = "Hello, I want help with my bill. The charges seem wrong."

# Step 2: Convert text to speech
tts = gTTS(text=text, lang='en')
tts.save("customer_query.mp3")

# Step 3: Load and confirm the audio file is created
audio = AudioSegment.from_mp3("customer_query.mp3")
print("MP3 file created successfully.")

from pydub import AudioSegment
import speech_recognition as sr

# Convert MP3 to WAV
audio = AudioSegment.from_mp3("customer_query.mp3")
audio.export("customer_query.wav", format="wav")

# Speech to Text
recognizer = sr.Recognizer()
with sr.AudioFile("customer_query.wav") as source:
    audio_data = recognizer.record(source)
    text = recognizer.recognize_google(audio_data)

print("Customer Said:", text)

import speech_recognition as sr

# Convert MP3 to WAV
audio.export("customer_query.wav", format="wav")

# Transcribe using SpeechRecognition
recognizer = sr.Recognizer()
with sr.AudioFile("customer_query.wav") as source:
    audio_data = recognizer.record(source)
    text = recognizer.recognize_google(audio_data)

print("Transcribed Text:", text)

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

nltk.download('punkt')
nltk.download('stopwords')

# Sample training data
queries = [
    "I want to pay my bill",
    "Why is my internet not working",
    "Can I speak to an agent",
    "I need help with account settings",
    "What is my current plan"
]

labels = ["billing", "technical", "general", "account", "general"]

# Vectorize and train
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(queries)
model = MultinomialNB()
model.fit(X, labels)

# Predict category of user's query
X_input = vectorizer.transform([text])
predicted = model.predict(X_input)
print("Query Category:", predicted[0])

import matplotlib.pyplot as plt

categories = ["billing", "technical", "general", "account"]
counts = [queries.count(label) for label in categories]

plt.bar(categories, counts)
plt.title("Query Distribution in Training Data")
plt.xlabel("Category")
plt.ylabel("Frequency")
plt.show()

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Load the WAV file you already exported (or use the MP3 if needed)
y, sr = librosa.load("customer_query.wav", sr=16000)

# === Mel Spectrogram ===
mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)

plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spectrogram_db, sr=sr, x_axis='time', y_axis='mel')
plt.colorbar(format='%+2.0f dB')
plt.title('Mel Spectrogram')
plt.tight_layout()
plt.show()

# === MFCC (Mel Frequency Cepstral Coefficients) ===
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

plt.figure(figsize=(10, 4))
librosa.display.specshow(mfccs, sr=sr, x_axis='time')
plt.colorbar()
plt.title('MFCCs')
plt.tight_layout()
plt.show()

# === Waveform ===
plt.figure(figsize=(10, 4))
plt.plot(y)
plt.title('Waveform of Audio Signal')
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.tight_layout()
plt.show()

chromagram = librosa.feature.chroma_stft(y=y, sr=sr)

plt.figure(figsize=(10, 4))
librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', cmap='coolwarm', sr=sr)
plt.colorbar()
plt.title('Chromagram')
plt.tight_layout()
plt.show()

spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
frames = range(len(spectral_centroids))
t = librosa.frames_to_time(frames)

plt.figure(figsize=(10, 4))
plt.semilogy(t, spectral_centroids, label='Spectral Centroid')
plt.xlabel('Time (s)')
plt.ylabel('Hz')
plt.title('Spectral Centroid (Perceived Brightness)')
plt.legend()
plt.tight_layout()
plt.show()